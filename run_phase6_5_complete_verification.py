#!/usr/bin/env python3
"""
Phase 6-5: å®Œå…¨æ¤œè¨¼å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
MCP Drone Control System ã®å…¨æ©Ÿèƒ½ãƒ»å“è³ªãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«ã‚’çµ±åˆå®Ÿè¡Œã—ã€æœ€çµ‚çš„ãªå“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™:
1. åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ (test_phase6_5_comprehensive.py)
2. APIä»•æ§˜é©åˆæ€§æ¤œè¨¼ (api_spec_validator.py)
3. ã‚·ã‚¹ãƒ†ãƒ å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯ (system_quality_checker.py)

æœ€çµ‚çš„ã«ã€ã‚·ã‚¹ãƒ†ãƒ ã®æœ¬ç•ªç’°å¢ƒå±•é–‹æº–å‚™çŠ¶æ³ã‚’ç·åˆè©•ä¾¡ã—ã¾ã™ã€‚
"""

import asyncio
import json
import sys
import subprocess
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import importlib.util


@dataclass
class VerificationResult:
    """æ¤œè¨¼çµæœ"""
    tool_name: str
    success: bool
    exit_code: int
    execution_time: float
    report_file: Optional[str] = None
    summary: Dict[str, Any] = None
    error_message: Optional[str] = None


@dataclass
class FinalAssessment:
    """æœ€çµ‚è©•ä¾¡"""
    overall_grade: str  # "A", "B", "C", "D", "F"
    overall_score: float  # 0-100
    production_ready: bool
    critical_issues: List[str]
    recommendations: List[str]
    verification_results: List[VerificationResult]
    total_execution_time: float


class Phase6CompleteVerificationRunner:
    """Phase 6-5 å®Œå…¨æ¤œè¨¼å®Ÿè¡Œãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.verification_results = []
        
        # å„æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã®æƒ…å ±
        self.verification_tools = [
            {
                "name": "åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ",
                "script": "test_phase6_5_comprehensive.py",
                "description": "å…¨ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆãƒ»å“è³ªä¿è¨¼"
            },
            {
                "name": "APIä»•æ§˜é©åˆæ€§æ¤œè¨¼",
                "script": "api_spec_validator.py", 
                "description": "OpenAPIä»•æ§˜ã¨ã®é©åˆæ€§æ¤œè¨¼"
            },
            {
                "name": "ã‚·ã‚¹ãƒ†ãƒ å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯",
                "script": "system_quality_checker.py",
                "description": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»å¯ç”¨æ€§ã®å“è³ªè©•ä¾¡"
            }
        ]
        
        # æœ¬ç•ªå±•é–‹åŸºæº–
        self.production_criteria = {
            "min_test_success_rate": 95.0,
            "min_api_compliance_rate": 90.0,
            "min_quality_score": 80.0,
            "max_critical_issues": 0,
            "max_high_issues": 2
        }
    
    async def run_complete_verification(self) -> FinalAssessment:
        """å®Œå…¨æ¤œè¨¼ã®å®Ÿè¡Œ"""
        print("ğŸš€ Phase 6-5: MCP Drone Control System å®Œå…¨æ¤œè¨¼é–‹å§‹")
        print("=" * 90)
        print("æœ¬ã‚·ã‚¹ãƒ†ãƒ ã®æœ¬ç•ªç’°å¢ƒå±•é–‹æº–å‚™çŠ¶æ³ã‚’åŒ…æ‹¬çš„ã«è©•ä¾¡ã—ã¾ã™")
        print("=" * 90)
        
        start_time = time.time()
        
        # ã‚·ã‚¹ãƒ†ãƒ å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
        await self._check_prerequisites()
        
        # å„æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œ
        for tool_info in self.verification_tools:
            print(f"\nğŸ“Š {tool_info['name']} å®Ÿè¡Œä¸­...")
            print(f"    {tool_info['description']}")
            print("-" * 70)
            
            result = await self._run_verification_tool(tool_info)
            self.verification_results.append(result)
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self._print_tool_result_summary(result)
        
        # æœ€çµ‚è©•ä¾¡ç”Ÿæˆ
        total_time = time.time() - start_time
        final_assessment = self._generate_final_assessment(total_time)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        self._print_final_assessment(final_assessment)
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        await self._generate_integrated_report(final_assessment)
        
        return final_assessment
    
    async def _check_prerequisites(self):
        """å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        print("\nğŸ” ã‚·ã‚¹ãƒ†ãƒ å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯...")
        
        # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        required_files = [
            "test_phase6_5_comprehensive.py",
            "api_spec_validator.py",
            "system_quality_checker.py",
            "shared/api-specs/mcp-api.yaml"
        ]
        
        missing_files = []
        for file_path in required_files:
            if not (self.project_root / file_path).exists():
                missing_files.append(file_path)
        
        if missing_files:
            print(f"âŒ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³ã—ã¦ã„ã¾ã™:")
            for file in missing_files:
                print(f"   â€¢ {file}")
            print("\næ¤œè¨¼ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            sys.exit(1)
        else:
            print("âœ… å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«: ã™ã¹ã¦å­˜åœ¨ç¢ºèª")
        
        # Pythonä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
        required_packages = ["aiohttp", "pyyaml", "jsonschema", "psutil"]
        missing_packages = []
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            print(f"âš ï¸ ä¸è¶³ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:")
            for package in missing_packages:
                print(f"   â€¢ {package}")
            print("   ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            print(f"   pip install {' '.join(missing_packages)}")
        else:
            print("âœ… Pythonä¾å­˜é–¢ä¿‚: ã™ã¹ã¦æº€è¶³")
        
        print("âœ… å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†")
    
    async def _run_verification_tool(self, tool_info: Dict[str, str]) -> VerificationResult:
        """å€‹åˆ¥æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œ"""
        start_time = time.time()
        script_path = self.project_root / tool_info["script"]
        
        try:
            # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
            process = await asyncio.create_subprocess_exec(
                sys.executable, str(script_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=str(self.project_root)
            )
            
            stdout, stderr = await process.communicate()
            execution_time = time.time() - start_time
            
            # å®Ÿè¡Œçµæœã®è§£æ
            success = process.returncode == 0
            
            # å‡ºåŠ›ã®è¡¨ç¤º
            if stdout:
                print(stdout.decode('utf-8'))
            if stderr and not success:
                print("ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:", stderr.decode('utf-8'))
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            report_file = self._find_report_file(tool_info["script"])
            summary = await self._extract_summary(report_file) if report_file else None
            
            return VerificationResult(
                tool_name=tool_info["name"],
                success=success,
                exit_code=process.returncode,
                execution_time=execution_time,
                report_file=report_file,
                summary=summary,
                error_message=stderr.decode('utf-8') if stderr else None
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return VerificationResult(
                tool_name=tool_info["name"],
                success=False,
                exit_code=-1,
                execution_time=execution_time,
                error_message=str(e)
            )
    
    def _find_report_file(self, script_name: str) -> Optional[str]:
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢"""
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆåã«åŸºã¥ã„ã¦ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨æ¸¬
        report_mappings = {
            "test_phase6_5_comprehensive.py": "phase6_5_test_report.json",
            "api_spec_validator.py": "api_compliance_report.json",
            "system_quality_checker.py": "system_quality_report.json"
        }
        
        report_file = report_mappings.get(script_name)
        if report_file and (self.project_root / report_file).exists():
            return report_file
        return None
    
    async def _extract_summary(self, report_file: str) -> Optional[Dict[str, Any]]:
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚µãƒãƒªãƒ¼æŠ½å‡º"""
        try:
            with open(self.project_root / report_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚µãƒãƒªãƒ¼æŠ½å‡º
            if "test_report" in data:
                return {
                    "type": "comprehensive_test",
                    "success_rate": data["test_report"]["success_rate"],
                    "total_tests": data["test_report"]["total_tests"],
                    "critical_issues": len(data["test_report"]["critical_issues"])
                }
            elif "api_compliance_report" in data:
                return {
                    "type": "api_compliance",
                    "compliance_rate": data["api_compliance_report"]["compliance_rate"],
                    "total_endpoints": data["api_compliance_report"]["total_endpoints"],
                    "issues": len(data["api_compliance_report"]["issues"])
                }
            elif "quality_report" in data:
                return {
                    "type": "quality_assessment",
                    "overall_score": data["quality_report"]["overall_score"],
                    "critical_issues": len([i for i in data["quality_report"]["issues"] if i["severity"] == "CRITICAL"])
                }
        except Exception as e:
            print(f"ãƒ¬ãƒãƒ¼ãƒˆè§£æã‚¨ãƒ©ãƒ¼ ({report_file}): {str(e)}")
        
        return None
    
    def _print_tool_result_summary(self, result: VerificationResult):
        """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        status = "âœ… æˆåŠŸ" if result.success else "âŒ å¤±æ•—"
        print(f"\nğŸ“Š {result.tool_name} å®Ÿè¡Œçµæœ:")
        print(f"   {status} (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.exit_code}, å®Ÿè¡Œæ™‚é–“: {result.execution_time:.2f}ç§’)")
        
        if result.summary:
            if result.summary["type"] == "comprehensive_test":
                print(f"   ğŸ¯ ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {result.summary['success_rate']:.1f}%")
                print(f"   ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {result.summary['total_tests']}")
                if result.summary['critical_issues'] > 0:
                    print(f"   ğŸš¨ é‡å¤§å•é¡Œ: {result.summary['critical_issues']}ä»¶")
            elif result.summary["type"] == "api_compliance":
                print(f"   ğŸ“‹ APIé©åˆç‡: {result.summary['compliance_rate']:.1f}%")
                print(f"   ğŸŒ ç·ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {result.summary['total_endpoints']}")
                if result.summary['issues'] > 0:
                    print(f"   âš ï¸ é©åˆæ€§å•é¡Œ: {result.summary['issues']}ä»¶")
            elif result.summary["type"] == "quality_assessment":
                print(f"   ğŸ¯ å“è³ªã‚¹ã‚³ã‚¢: {result.summary['overall_score']:.1f}/100")
                if result.summary['critical_issues'] > 0:
                    print(f"   ğŸš¨ é‡å¤§å“è³ªå•é¡Œ: {result.summary['critical_issues']}ä»¶")
        
        if result.error_message and not result.success:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {result.error_message[:100]}...")
    
    def _generate_final_assessment(self, total_time: float) -> FinalAssessment:
        """æœ€çµ‚è©•ä¾¡ç”Ÿæˆ"""
        # å„æ¤œè¨¼çµæœã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
        test_success_rate = 0.0
        api_compliance_rate = 0.0
        quality_score = 0.0
        critical_issues = []
        recommendations = []
        
        for result in self.verification_results:
            if result.summary:
                if result.summary["type"] == "comprehensive_test":
                    test_success_rate = result.summary["success_rate"]
                    if result.summary["critical_issues"] > 0:
                        critical_issues.append(f"ãƒ†ã‚¹ãƒˆã§{result.summary['critical_issues']}ä»¶ã®é‡å¤§å•é¡Œã‚’æ¤œå‡º")
                elif result.summary["type"] == "api_compliance":
                    api_compliance_rate = result.summary["compliance_rate"]
                    if result.summary["compliance_rate"] < 95:
                        critical_issues.append(f"APIé©åˆç‡ãŒ{result.summary['compliance_rate']:.1f}%ã§åŸºæº–ã‚’ä¸‹å›ã‚‹")
                elif result.summary["type"] == "quality_assessment":
                    quality_score = result.summary["overall_score"]
                    if result.summary["critical_issues"] > 0:
                        critical_issues.append(f"å“è³ªè©•ä¾¡ã§{result.summary['critical_issues']}ä»¶ã®é‡å¤§å•é¡Œã‚’æ¤œå‡º")
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        weights = {
            "test_success": 0.4,
            "api_compliance": 0.3,
            "quality_score": 0.3
        }
        
        overall_score = (
            test_success_rate * weights["test_success"] +
            api_compliance_rate * weights["api_compliance"] +
            quality_score * weights["quality_score"]
        )
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰æ±ºå®š
        if overall_score >= 90:
            grade = "A"
        elif overall_score >= 80:
            grade = "B"
        elif overall_score >= 70:
            grade = "C"
        elif overall_score >= 60:
            grade = "D"
        else:
            grade = "F"
        
        # æœ¬ç•ªç’°å¢ƒæº–å‚™çŠ¶æ³åˆ¤å®š
        production_ready = (
            test_success_rate >= self.production_criteria["min_test_success_rate"] and
            api_compliance_rate >= self.production_criteria["min_api_compliance_rate"] and
            quality_score >= self.production_criteria["min_quality_score"] and
            len(critical_issues) <= self.production_criteria["max_critical_issues"]
        )
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        if not production_ready:
            recommendations.append("æœ¬ç•ªç’°å¢ƒå±•é–‹å‰ã«é‡å¤§å•é¡Œã®è§£æ±ºãŒå¿…è¦ã§ã™")
        
        if test_success_rate < 95:
            recommendations.append("ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ã‚’95%ä»¥ä¸Šã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        
        if api_compliance_rate < 95:
            recommendations.append("APIä»•æ§˜é©åˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        
        if quality_score < 85:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ å“è³ªã®å…¨èˆ¬çš„ãªæ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™")
        
        if not critical_issues:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ã¯é«˜å“è³ªã§æœ¬ç•ªç’°å¢ƒã«é©ã—ã¦ã„ã¾ã™")
        
        return FinalAssessment(
            overall_grade=grade,
            overall_score=overall_score,
            production_ready=production_ready,
            critical_issues=critical_issues,
            recommendations=recommendations,
            verification_results=self.verification_results,
            total_execution_time=total_time
        )
    
    def _print_final_assessment(self, assessment: FinalAssessment):
        """æœ€çµ‚è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print("\n" + "=" * 90)
        print("ğŸ† Phase 6-5: MCP Drone Control System æœ€çµ‚è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 90)
        
        # ç·åˆè©•ä¾¡
        grade_colors = {
            "A": "ğŸŸ¢", "B": "ğŸŸ¡", "C": "ğŸŸ ", "D": "ğŸ”´", "F": "âš«"
        }
        grade_icon = grade_colors.get(assessment.overall_grade, "âšª")
        
        print(f"\nğŸ“Š ç·åˆè©•ä¾¡:")
        print(f"   {grade_icon} ã‚°ãƒ¬ãƒ¼ãƒ‰: {assessment.overall_grade}")
        print(f"   ğŸ“ˆ ç·åˆã‚¹ã‚³ã‚¢: {assessment.overall_score:.1f}/100")
        print(f"   â±ï¸ ç·æ¤œè¨¼æ™‚é–“: {assessment.total_execution_time:.2f}ç§’")
        
        # æœ¬ç•ªç’°å¢ƒæº–å‚™çŠ¶æ³
        print(f"\nğŸš€ æœ¬ç•ªç’°å¢ƒå±•é–‹æº–å‚™çŠ¶æ³:")
        if assessment.production_ready:
            print("   âœ… æœ¬ç•ªç’°å¢ƒå±•é–‹æº–å‚™å®Œäº†")
        else:
            print("   âŒ æœ¬ç•ªç’°å¢ƒå±•é–‹æº–å‚™æœªå®Œäº†")
        
        # æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“‹ æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼:")
        for result in assessment.verification_results:
            status = "âœ…" if result.success else "âŒ"
            print(f"   {status} {result.tool_name}: {result.execution_time:.2f}ç§’")
        
        # é‡å¤§å•é¡Œ
        if assessment.critical_issues:
            print(f"\nğŸš¨ é‡å¤§å•é¡Œ ({len(assessment.critical_issues)}ä»¶):")
            for issue in assessment.critical_issues:
                print(f"   ğŸš¨ {issue}")
        
        # æ¨å¥¨äº‹é …
        if assessment.recommendations:
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for rec in assessment.recommendations:
                print(f"   ğŸ’¡ {rec}")
        
        # æœ€çµ‚åˆ¤å®š
        print(f"\nğŸ¯ æœ€çµ‚åˆ¤å®š:")
        if assessment.overall_grade == "A":
            print("   ğŸŸ¢ å„ªç§€ - ã‚·ã‚¹ãƒ†ãƒ ã¯éå¸¸ã«é«˜å“è³ªã§æœ¬ç•ªç’°å¢ƒã«æœ€é©ã§ã™")
        elif assessment.overall_grade == "B":
            print("   ğŸŸ¡ è‰¯å¥½ - ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ãªå“è³ªã§æœ¬ç•ªç’°å¢ƒã«é©ã—ã¦ã„ã¾ã™")
        elif assessment.overall_grade == "C":
            print("   ğŸŸ  åˆæ ¼ - ä¸€éƒ¨æ”¹å–„ãŒå¿…è¦ã§ã™ãŒæœ¬ç•ªç’°å¢ƒå±•é–‹å¯èƒ½ã§ã™")
        elif assessment.overall_grade == "D":
            print("   ğŸ”´ è¦æ”¹å–„ - é‡è¦ãªå•é¡ŒãŒã‚ã‚Šæ”¹å–„å¾Œã®å±•é–‹ã‚’æ¨å¥¨ã—ã¾ã™")
        else:
            print("   âš« ä¸åˆæ ¼ - é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šæœ¬ç•ªç’°å¢ƒå±•é–‹ã¯æ¨å¥¨ã—ã¾ã›ã‚“")
        
        print("\n" + "=" * 90)
    
    async def _generate_integrated_report(self, assessment: FinalAssessment):
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # å„æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆ
        integrated_data = {
            "phase6_5_final_assessment": {
                "overall_grade": assessment.overall_grade,
                "overall_score": assessment.overall_score,
                "production_ready": assessment.production_ready,
                "critical_issues": assessment.critical_issues,
                "recommendations": assessment.recommendations,
                "total_execution_time": assessment.total_execution_time
            },
            "verification_results": [
                {
                    "tool_name": result.tool_name,
                    "success": result.success,
                    "exit_code": result.exit_code,
                    "execution_time": result.execution_time,
                    "report_file": result.report_file,
                    "summary": result.summary
                }
                for result in assessment.verification_results
            ],
            "detailed_reports": {},
            "timestamp": datetime.now().isoformat(),
            "system": "MCP Drone Control System",
            "phase": "6-5",
            "verification_scope": "Complete System Verification"
        }
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆ
        for result in assessment.verification_results:
            if result.report_file:
                try:
                    with open(self.project_root / result.report_file, 'r', encoding='utf-8') as f:
                        detailed_report = json.load(f)
                    integrated_data["detailed_reports"][result.tool_name] = detailed_report
                except Exception as e:
                    print(f"è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆã‚¨ãƒ©ãƒ¼ ({result.report_file}): {str(e)}")
        
        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        output_file = "phase6_5_final_verification_report.json"
        with open(self.project_root / output_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ çµ±åˆæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {output_file}")
        
        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã‚‚ç”Ÿæˆ
        await self._generate_executive_summary(assessment)
    
    async def _generate_executive_summary(self, assessment: FinalAssessment):
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        summary_content = f"""# MCP Drone Control System - Phase 6-5 æœ€çµ‚æ¤œè¨¼ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

## æ¤œè¨¼æ¦‚è¦
å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
æ¤œè¨¼æ™‚é–“: {assessment.total_execution_time:.2f}ç§’
æ¤œè¨¼ç¯„å›²: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®åŒ…æ‹¬çš„å“è³ªãƒ»æ©Ÿèƒ½ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼

## ç·åˆè©•ä¾¡
- **ã‚°ãƒ¬ãƒ¼ãƒ‰**: {assessment.overall_grade}
- **ç·åˆã‚¹ã‚³ã‚¢**: {assessment.overall_score:.1f}/100
- **æœ¬ç•ªç’°å¢ƒæº–å‚™çŠ¶æ³**: {"âœ… æº–å‚™å®Œäº†" if assessment.production_ready else "âŒ æº–å‚™æœªå®Œäº†"}

## æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼
"""
        
        for result in assessment.verification_results:
            status = "âœ… æˆåŠŸ" if result.success else "âŒ å¤±æ•—"
            summary_content += f"- **{result.tool_name}**: {status} ({result.execution_time:.2f}ç§’)\n"
            if result.summary:
                if result.summary["type"] == "comprehensive_test":
                    summary_content += f"  - ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {result.summary['success_rate']:.1f}%\n"
                elif result.summary["type"] == "api_compliance":
                    summary_content += f"  - APIé©åˆç‡: {result.summary['compliance_rate']:.1f}%\n"
                elif result.summary["type"] == "quality_assessment":
                    summary_content += f"  - å“è³ªã‚¹ã‚³ã‚¢: {result.summary['overall_score']:.1f}/100\n"
        
        if assessment.critical_issues:
            summary_content += f"\n## é‡å¤§å•é¡Œ ({len(assessment.critical_issues)}ä»¶)\n"
            for issue in assessment.critical_issues:
                summary_content += f"- {issue}\n"
        
        summary_content += f"\n## æ¨å¥¨äº‹é …\n"
        for rec in assessment.recommendations:
            summary_content += f"- {rec}\n"
        
        summary_content += f"\n## æœ€çµ‚åˆ¤å®š\n"
        if assessment.overall_grade == "A":
            summary_content += "**å„ªç§€** - ã‚·ã‚¹ãƒ†ãƒ ã¯éå¸¸ã«é«˜å“è³ªã§æœ¬ç•ªç’°å¢ƒã«æœ€é©ã§ã™ã€‚\n"
        elif assessment.overall_grade == "B":
            summary_content += "**è‰¯å¥½** - ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ãªå“è³ªã§æœ¬ç•ªç’°å¢ƒã«é©ã—ã¦ã„ã¾ã™ã€‚\n"
        elif assessment.overall_grade == "C":
            summary_content += "**åˆæ ¼** - ä¸€éƒ¨æ”¹å–„ãŒå¿…è¦ã§ã™ãŒæœ¬ç•ªç’°å¢ƒå±•é–‹å¯èƒ½ã§ã™ã€‚\n"
        elif assessment.overall_grade == "D":
            summary_content += "**è¦æ”¹å–„** - é‡è¦ãªå•é¡ŒãŒã‚ã‚Šæ”¹å–„å¾Œã®å±•é–‹ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n"
        else:
            summary_content += "**ä¸åˆæ ¼** - é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šæœ¬ç•ªç’°å¢ƒå±•é–‹ã¯æ¨å¥¨ã—ã¾ã›ã‚“ã€‚\n"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        with open(self.project_root / "PHASE6_5_EXECUTIVE_SUMMARY.md", 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        print(f"ğŸ“„ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: PHASE6_5_EXECUTIVE_SUMMARY.md")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Phase 6-5: MCP Drone Control System å®Œå…¨æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    print("Complete System Verification and Production Readiness Assessment")
    print("=" * 90)
    
    # å®Œå…¨æ¤œè¨¼ãƒ©ãƒ³ãƒŠãƒ¼åˆæœŸåŒ–
    runner = Phase6CompleteVerificationRunner()
    
    # å®Œå…¨æ¤œè¨¼å®Ÿè¡Œ
    final_assessment = await runner.run_complete_verification()
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
    if final_assessment.production_ready and final_assessment.overall_grade in ["A", "B"]:
        return 0  # æœ¬ç•ªç’°å¢ƒæº–å‚™å®Œäº†
    elif final_assessment.overall_grade in ["C", "D"]:
        return 1  # æ”¹å–„å¿…è¦
    else:
        return 2  # é‡å¤§å•é¡Œ


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    print(f"\nğŸ Phase 6-5 å®Œå…¨æ¤œè¨¼å®Œäº† (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code})")
    sys.exit(exit_code)