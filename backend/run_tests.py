#!/usr/bin/env python3
"""
Tello EDU Dummy System Test Runner
åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import subprocess
import sys
import time
from pathlib import Path
from typing import List, Dict, Any


class TestRunner:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.backend_dir = Path(__file__).parent
        self.test_categories = {
            'unit': {
                'description': 'å˜ä½“ãƒ†ã‚¹ãƒˆï¼ˆæ—¢å­˜ã®Phase2ãƒ»Phase3ãƒ†ã‚¹ãƒˆï¼‰',
                'files': [
                    'tests/test_drone_simulator.py',
                    'tests/test_virtual_camera.py'
                ],
                'markers': 'unit'
            },
            'integration': {
                'description': 'çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆPhase2â†”Phase3ã€è¨­å®šçµ±åˆã€ãƒ•ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼‰',
                'files': [
                    'tests/test_integration_phase2_phase3.py',
                    'tests/test_configuration_integration.py',
                    'tests/test_full_workflow_integration.py'
                ],
                'markers': 'integration'
            },
            'edge_cases': {
                'description': 'ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ»å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ',
                'files': [
                    'tests/test_edge_cases.py'
                ],
                'markers': 'edge_case'
            },
            'error_handling': {
                'description': 'ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ä¾‹å¤–å‡¦ç†ãƒ†ã‚¹ãƒˆ',
                'files': [
                    'tests/test_error_handling.py'
                ],
                'markers': 'error_handling'
            },
            'performance': {
                'description': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»é•·æ™‚é–“ç¨¼åƒãƒ†ã‚¹ãƒˆ',
                'files': [
                    'tests/test_performance.py'
                ],
                'markers': 'performance'
            },
            'compatibility': {
                'description': 'ç’°å¢ƒäº’æ›æ€§ãƒ»ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ',
                'files': [
                    'tests/test_compatibility.py'
                ],
                'markers': 'compatibility'
            }
        }
    
    def run_category(self, category: str, verbose: bool = False, coverage: bool = True) -> bool:
        """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        if category not in self.test_categories:
            print(f"âŒ Unknown test category: {category}")
            return False
        
        category_info = self.test_categories[category]
        print(f"\nğŸ§ª Running {category_info['description']}")
        print(f"Files: {', '.join(category_info['files'])}")
        
        cmd = ['python', '-m', 'pytest']
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š
        for test_file in category_info['files']:
            if (self.backend_dir / test_file).exists():
                cmd.append(test_file)
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ 
        if verbose:
            cmd.append('-v')
        
        if coverage:
            cmd.extend(['--cov=src', '--cov-report=term-missing'])
        
        # ãƒãƒ¼ã‚«ãƒ¼æŒ‡å®š
        if category_info['markers']:
            cmd.extend(['-m', category_info['markers']])
        
        # å®Ÿè¡Œ
        start_time = time.time()
        try:
            result = subprocess.run(cmd, cwd=self.backend_dir, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… {category} tests passed")
            else:
                print(f"âŒ {category} tests failed")
                if verbose:
                    print("STDOUT:", result.stdout)
                    print("STDERR:", result.stderr)
            
            elapsed = time.time() - start_time
            print(f"â±ï¸  Time: {elapsed:.2f}s")
            
            return result.returncode == 0
            
        except Exception as e:
            print(f"âŒ Error running {category} tests: {e}")
            return False
    
    def run_all(self, exclude_slow: bool = True, verbose: bool = False) -> Dict[str, bool]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªã‚’å®Ÿè¡Œ"""
        print("ğŸš€ Running Tello EDU Dummy System Comprehensive Test Suite")
        print("=" * 60)
        
        results = {}
        total_start = time.time()
        
        for category in self.test_categories.keys():
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€é™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            if exclude_slow and category == 'performance':
                print(f"\nâ­ï¸  Skipping {category} tests (slow tests excluded)")
                results[category] = None
                continue
            
            success = self.run_category(category, verbose=verbose)
            results[category] = success
        
        total_time = time.time() - total_start
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\nğŸ“Š Test Results Summary")
        print("=" * 40)
        
        passed = sum(1 for r in results.values() if r is True)
        failed = sum(1 for r in results.values() if r is False)
        skipped = sum(1 for r in results.values() if r is None)
        
        for category, result in results.items():
            if result is True:
                print(f"âœ… {category:15} PASSED")
            elif result is False:
                print(f"âŒ {category:15} FAILED")
            else:
                print(f"â­ï¸  {category:15} SKIPPED")
        
        print(f"\nğŸ“ˆ Total: {passed} passed, {failed} failed, {skipped} skipped")
        print(f"â±ï¸  Total time: {total_time:.2f}s")
        
        return results
    
    def run_quick(self) -> bool:
        """ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆå˜ä½“ãƒ†ã‚¹ãƒˆã®ã¿ï¼‰"""
        print("âš¡ Running Quick Tests (Unit Tests Only)")
        return self.run_category('unit', verbose=True)
    
    def run_coverage_report(self) -> bool:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸ“Š Generating Coverage Report")
        
        cmd = [
            'python', '-m', 'pytest',
            '--cov=src',
            '--cov-report=html:htmlcov',
            '--cov-report=xml',
            '--cov-report=term-missing',
            'tests/'
        ]
        
        try:
            result = subprocess.run(cmd, cwd=self.backend_dir)
            if result.returncode == 0:
                print("âœ… Coverage report generated in htmlcov/")
                return True
            else:
                print("âŒ Coverage report generation failed")
                return False
        except Exception as e:
            print(f"âŒ Error generating coverage report: {e}")
            return False
    
    def list_categories(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªä¸€è¦§è¡¨ç¤º"""
        print("ğŸ“‹ Available Test Categories:")
        print("=" * 50)
        
        for category, info in self.test_categories.items():
            print(f"  {category:15} - {info['description']}")
            for file in info['files']:
                exists = "âœ…" if (self.backend_dir / file).exists() else "âŒ"
                print(f"    {exists} {file}")
            print()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Tello EDU Dummy System Test Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_tests.py --all                # Run all tests
  python run_tests.py --quick              # Run only unit tests
  python run_tests.py --category integration  # Run integration tests
  python run_tests.py --coverage           # Generate coverage report
  python run_tests.py --list               # List available categories
        """
    )
    
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--all', action='store_true', 
                      help='Run all test categories')
    group.add_argument('--quick', action='store_true',
                      help='Run quick tests (unit tests only)')
    group.add_argument('--category', choices=['unit', 'integration', 'edge_cases', 
                                             'error_handling', 'performance', 'compatibility'],
                      help='Run specific test category')
    group.add_argument('--coverage', action='store_true',
                      help='Generate coverage report')
    group.add_argument('--list', action='store_true',
                      help='List available test categories')
    
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Verbose output')
    parser.add_argument('--include-slow', action='store_true',
                       help='Include slow tests (like performance tests)')
    
    args = parser.parse_args()
    
    runner = TestRunner()
    
    if args.list:
        runner.list_categories()
        return 0
    
    if args.coverage:
        success = runner.run_coverage_report()
        return 0 if success else 1
    
    if args.quick:
        success = runner.run_quick()
        return 0 if success else 1
    
    if args.category:
        success = runner.run_category(args.category, verbose=args.verbose)
        return 0 if success else 1
    
    if args.all:
        results = runner.run_all(exclude_slow=not args.include_slow, verbose=args.verbose)
        failed_count = sum(1 for r in results.values() if r is False)
        return 0 if failed_count == 0 else 1
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ˜ãƒ«ãƒ—è¡¨ç¤º
    parser.print_help()
    return 0


if __name__ == '__main__':
    sys.exit(main())