#!/usr/bin/env python3
"""
Phase 9: Performance Benchmark Tests
MCPã‚µãƒ¼ãƒãƒ¼ã®Pythonç‰ˆã¨Node.jsç‰ˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°æ¯”è¼ƒ

ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é …ç›®:
1. åŸºæœ¬çš„ãªå¿œç­”æ™‚é–“æ¸¬å®š
2. ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ
4. CPUä½¿ç”¨ç‡æ¯”è¼ƒ
5. åŒæ™‚æ¥ç¶šè² è·ãƒ†ã‚¹ãƒˆ
6. é•·æ™‚é–“ç¨¼åƒå®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import json
import os
import time
import psutil
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

import aiohttp
import matplotlib.pyplot as plt
import numpy as np


class BenchmarkCategory(Enum):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒª"""
    RESPONSE_TIME = "response_time"
    THROUGHPUT = "throughput"
    MEMORY_USAGE = "memory_usage"
    CPU_USAGE = "cpu_usage"
    CONCURRENCY = "concurrency"
    STABILITY = "stability"


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    category: BenchmarkCategory
    test_name: str
    python_metrics: Dict[str, Any]
    nodejs_metrics: Dict[str, Any]
    winner: str  # "python", "nodejs", "tie"
    performance_ratio: float  # nodejs/pythonæ¯”ç‡
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceReport:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ"""
    python_wins: int = 0
    nodejs_wins: int = 0
    ties: int = 0
    total_benchmarks: int = 0
    results: List[BenchmarkResult] = field(default_factory=list)
    summary_stats: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)


class MCPPerformanceBenchmark:
    """MCPãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.report = PerformanceReport()
        
        # ã‚µãƒ¼ãƒãƒ¼URLè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°å¯¾å¿œï¼‰
        self.python_server_url = f"http://localhost:{os.environ.get('MCP_PYTHON_PORT', '8001')}"  # ãƒ¬ã‚¬ã‚·ãƒ¼Pythonç‰ˆ  
        self.nodejs_server_url = f"http://localhost:{os.environ.get('MCP_NODEJS_PORT', '3001')}"  # ãƒ¡ã‚¤ãƒ³Node.jsç‰ˆ
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
        self.benchmark_config = {
            "response_time_samples": 100,
            "throughput_duration": 30,  # ç§’
            "concurrency_levels": [1, 5, 10, 20, 50],
            "stability_duration": 300,  # 5åˆ†
            "memory_sampling_interval": 5  # ç§’
        }
    
    async def run_performance_benchmarks(self) -> PerformanceReport:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸš€ Phase 9: Performance Benchmark - Python vs Node.js")
        print("=" * 80)
        
        start_time = time.time()
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        await self._benchmark_response_time()
        await self._benchmark_throughput()
        await self._benchmark_memory_usage()
        await self._benchmark_cpu_usage()
        await self._benchmark_concurrency()
        await self._benchmark_stability()
        
        # çµæœåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._analyze_results()
        self._generate_recommendations()
        
        execution_time = time.time() - start_time
        print(f"\nâœ… å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        
        return self.report
    
    async def _benchmark_response_time(self):
        """å¿œç­”æ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nâ±ï¸ å¿œç­”æ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¿œç­”æ™‚é–“æ¸¬å®š
        python_times = await self._measure_response_times(
            f"{self.python_server_url}/mcp/system/health",
            self.benchmark_config["response_time_samples"]
        )
        
        nodejs_times = await self._measure_response_times(
            f"{self.nodejs_server_url}/mcp/system/health",
            self.benchmark_config["response_time_samples"]
        )
        
        # çµ±è¨ˆè¨ˆç®—
        python_stats = self._calculate_time_stats(python_times)
        nodejs_stats = self._calculate_time_stats(nodejs_times)
        
        # å‹è€…æ±ºå®š
        winner = "nodejs" if nodejs_stats["mean"] < python_stats["mean"] else "python"
        if abs(nodejs_stats["mean"] - python_stats["mean"]) < 0.01:  # 10msä»¥å†…ã¯åŒç­‰
            winner = "tie"
        
        performance_ratio = nodejs_stats["mean"] / python_stats["mean"] if python_stats["mean"] > 0 else 1.0
        
        result = BenchmarkResult(
            category=BenchmarkCategory.RESPONSE_TIME,
            test_name="Health Check Response Time",
            python_metrics=python_stats,
            nodejs_metrics=nodejs_stats,
            winner=winner,
            performance_ratio=performance_ratio,
            details={"python_times": python_times, "nodejs_times": nodejs_times}
        )
        
        self._add_result(result)
        print(f"  Pythonå¹³å‡å¿œç­”æ™‚é–“: {python_stats['mean']:.3f}s (Â±{python_stats['std']:.3f})")
        print(f"  Node.jså¹³å‡å¿œç­”æ™‚é–“: {nodejs_stats['mean']:.3f}s (Â±{nodejs_stats['std']:.3f})")
        print(f"  å‹è€…: {winner}")
    
    async def _benchmark_throughput(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ“Š ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        duration = self.benchmark_config["throughput_duration"]
        
        # Pythonç‰ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
        python_rps = await self._measure_throughput(
            f"{self.python_server_url}/mcp/system/health", duration
        )
        
        # Node.jsç‰ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
        nodejs_rps = await self._measure_throughput(
            f"{self.nodejs_server_url}/mcp/system/health", duration
        )
        
        python_metrics = {"requests_per_second": python_rps, "total_requests": python_rps * duration}
        nodejs_metrics = {"requests_per_second": nodejs_rps, "total_requests": nodejs_rps * duration}
        
        winner = "nodejs" if nodejs_rps > python_rps else "python"
        if abs(nodejs_rps - python_rps) < (max(nodejs_rps, python_rps) * 0.05):  # 5%ä»¥å†…ã¯åŒç­‰
            winner = "tie"
        
        performance_ratio = nodejs_rps / python_rps if python_rps > 0 else 1.0
        
        result = BenchmarkResult(
            category=BenchmarkCategory.THROUGHPUT,
            test_name="Request Throughput",
            python_metrics=python_metrics,
            nodejs_metrics=nodejs_metrics,
            winner=winner,
            performance_ratio=performance_ratio
        )
        
        self._add_result(result)
        print(f"  Python RPS: {python_rps:.2f}")
        print(f"  Node.js RPS: {nodejs_rps:.2f}")
        print(f"  å‹è€…: {winner}")
    
    async def _benchmark_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…ã®ã¿ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ãŒå¿…è¦ï¼‰
        python_metrics = {"note": "Memory monitoring requires process identification"}
        nodejs_metrics = {"note": "Memory monitoring requires process identification"}
        
        result = BenchmarkResult(
            category=BenchmarkCategory.MEMORY_USAGE,
            test_name="Memory Usage Comparison",
            python_metrics=python_metrics,
            nodejs_metrics=nodejs_metrics,
            winner="tie",
            performance_ratio=1.0,
            details={"implementation_note": "Requires process monitoring setup"}
        )
        
        self._add_result(result)
        print("  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š: ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã®å®Ÿè£…ãŒå¿…è¦")
    
    async def _benchmark_cpu_usage(self):
        """CPUä½¿ç”¨ç‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ–¥ï¸ CPUä½¿ç”¨ç‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        # ç¾åœ¨ã¯åŸºæœ¬çš„ãªå®Ÿè£…ã®ã¿
        python_metrics = {"note": "CPU monitoring requires process identification"}
        nodejs_metrics = {"note": "CPU monitoring requires process identification"}
        
        result = BenchmarkResult(
            category=BenchmarkCategory.CPU_USAGE,
            test_name="CPU Usage Comparison",
            python_metrics=python_metrics,
            nodejs_metrics=nodejs_metrics,
            winner="tie",
            performance_ratio=1.0
        )
        
        self._add_result(result)
        print("  CPUä½¿ç”¨ç‡æ¸¬å®š: ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã®å®Ÿè£…ãŒå¿…è¦")
    
    async def _benchmark_concurrency(self):
        """åŒæ™‚æ¥ç¶šè² è·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ”€ åŒæ™‚æ¥ç¶šè² è·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        concurrency_levels = self.benchmark_config["concurrency_levels"]
        python_results = {}
        nodejs_results = {}
        
        for concurrency in concurrency_levels:
            print(f"  åŒæ™‚æ¥ç¶šæ•° {concurrency} ãƒ†ã‚¹ãƒˆä¸­...")
            
            # Pythonç‰ˆãƒ†ã‚¹ãƒˆ
            python_success_rate = await self._test_concurrency_level(
                f"{self.python_server_url}/mcp/system/health", concurrency
            )
            
            # Node.jsç‰ˆãƒ†ã‚¹ãƒˆ
            nodejs_success_rate = await self._test_concurrency_level(
                f"{self.nodejs_server_url}/mcp/system/health", concurrency
            )
            
            python_results[f"concurrency_{concurrency}"] = python_success_rate
            nodejs_results[f"concurrency_{concurrency}"] = nodejs_success_rate
            
            print(f"    PythonæˆåŠŸç‡: {python_success_rate:.1%}")
            print(f"    Node.jsæˆåŠŸç‡: {nodejs_success_rate:.1%}")
        
        # å…¨ä½“çš„ãªæˆåŠŸç‡è¨ˆç®—
        python_avg = statistics.mean(python_results.values())
        nodejs_avg = statistics.mean(nodejs_results.values())
        
        winner = "nodejs" if nodejs_avg > python_avg else "python"
        if abs(nodejs_avg - python_avg) < 0.05:  # 5%ä»¥å†…ã¯åŒç­‰
            winner = "tie"
        
        performance_ratio = nodejs_avg / python_avg if python_avg > 0 else 1.0
        
        result = BenchmarkResult(
            category=BenchmarkCategory.CONCURRENCY,
            test_name="Concurrent Request Handling",
            python_metrics={"average_success_rate": python_avg, "results_by_level": python_results},
            nodejs_metrics={"average_success_rate": nodejs_avg, "results_by_level": nodejs_results},
            winner=winner,
            performance_ratio=performance_ratio
        )
        
        self._add_result(result)
        print(f"  Pythonå¹³å‡æˆåŠŸç‡: {python_avg:.1%}")
        print(f"  Node.jså¹³å‡æˆåŠŸç‡: {nodejs_avg:.1%}")
        print(f"  å‹è€…: {winner}")
    
    async def _benchmark_stability(self):
        """é•·æ™‚é–“ç¨¼åƒå®‰å®šæ€§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸƒ é•·æ™‚é–“ç¨¼åƒå®‰å®šæ€§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
        
        duration = self.benchmark_config["stability_duration"]
        print(f"  {duration}ç§’é–“ã®å®‰å®šæ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        # ç°¡æ˜“ç‰ˆ: å®šæœŸçš„ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        python_stability = await self._test_stability(
            f"{self.python_server_url}/mcp/system/health", duration
        )
        
        nodejs_stability = await self._test_stability(
            f"{self.nodejs_server_url}/mcp/system/health", duration
        )
        
        winner = "nodejs" if nodejs_stability["success_rate"] > python_stability["success_rate"] else "python"
        if abs(nodejs_stability["success_rate"] - python_stability["success_rate"]) < 0.02:  # 2%ä»¥å†…ã¯åŒç­‰
            winner = "tie"
        
        performance_ratio = nodejs_stability["success_rate"] / python_stability["success_rate"] if python_stability["success_rate"] > 0 else 1.0
        
        result = BenchmarkResult(
            category=BenchmarkCategory.STABILITY,
            test_name="Long-term Stability",
            python_metrics=python_stability,
            nodejs_metrics=nodejs_stability,
            winner=winner,
            performance_ratio=performance_ratio
        )
        
        self._add_result(result)
        print(f"  Pythonå®‰å®šæ€§: {python_stability['success_rate']:.1%}")
        print(f"  Node.jså®‰å®šæ€§: {nodejs_stability['success_rate']:.1%}")
        print(f"  å‹è€…: {winner}")
    
    async def _measure_response_times(self, url: str, samples: int) -> List[float]:
        """å¿œç­”æ™‚é–“æ¸¬å®š"""
        times = []
        
        async with aiohttp.ClientSession() as session:
            for i in range(samples):
                try:
                    start_time = time.time()
                    async with session.get(url) as response:
                        end_time = time.time()
                        if response.status == 200:
                            times.append(end_time - start_time)
                        else:
                            # ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚‚æ™‚é–“ã¨ã—ã¦è¨˜éŒ²ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£ã¨ã—ã¦é«˜ã„å€¤ã‚’è¨­å®šï¼‰
                            times.append((end_time - start_time) * 2)
                except:
                    # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£æ™‚é–“ã‚’è¿½åŠ 
                    times.append(5.0)
                
                # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ã™ããªã„ã‚ˆã†å°ã•ãªå¾…æ©Ÿ
                if i < samples - 1:
                    await asyncio.sleep(0.01)
        
        return times
    
    def _calculate_time_stats(self, times: List[float]) -> Dict[str, float]:
        """æ™‚é–“çµ±è¨ˆè¨ˆç®—"""
        if not times:
            return {"mean": 0, "median": 0, "std": 0, "min": 0, "max": 0, "p95": 0, "p99": 0}
        
        times_sorted = sorted(times)
        n = len(times)
        
        return {
            "mean": statistics.mean(times),
            "median": statistics.median(times),
            "std": statistics.stdev(times) if n > 1 else 0,
            "min": min(times),
            "max": max(times),
            "p95": times_sorted[int(n * 0.95)] if n > 0 else 0,
            "p99": times_sorted[int(n * 0.99)] if n > 0 else 0
        }
    
    async def _measure_throughput(self, url: str, duration: int) -> float:
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š"""
        end_time = time.time() + duration
        request_count = 0
        
        async with aiohttp.ClientSession() as session:
            while time.time() < end_time:
                try:
                    async with session.get(url) as response:
                        request_count += 1
                except:
                    pass
                
                # å°ã•ãªå¾…æ©Ÿã‚’å…¥ã‚Œã¦ã‚µãƒ¼ãƒãƒ¼éè² è·ã‚’é˜²ã
                await asyncio.sleep(0.001)
        
        return request_count / duration
    
    async def _test_concurrency_level(self, url: str, concurrency: int) -> float:
        """åŒæ™‚æ¥ç¶šãƒ¬ãƒ™ãƒ«ãƒ†ã‚¹ãƒˆ"""
        tasks = []
        
        async with aiohttp.ClientSession() as session:
            for _ in range(concurrency):
                task = session.get(url)
                tasks.append(task)
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            successful_requests = 0
            for response in responses:
                if not isinstance(response, Exception):
                    if hasattr(response, 'status') and response.status == 200:
                        successful_requests += 1
            
            return successful_requests / concurrency
    
    async def _test_stability(self, url: str, duration: int) -> Dict[str, Any]:
        """å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ"""
        end_time = time.time() + duration
        total_requests = 0
        successful_requests = 0
        error_count = 0
        
        async with aiohttp.ClientSession() as session:
            while time.time() < end_time:
                try:
                    async with session.get(url) as response:
                        total_requests += 1
                        if response.status == 200:
                            successful_requests += 1
                        else:
                            error_count += 1
                except Exception as e:
                    total_requests += 1
                    error_count += 1
                
                # å®šæœŸçš„ãªãƒ†ã‚¹ãƒˆé–“éš”
                await asyncio.sleep(1)
        
        success_rate = successful_requests / total_requests if total_requests > 0 else 0
        
        return {
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "error_count": error_count,
            "success_rate": success_rate,
            "test_duration": duration
        }
    
    def _add_result(self, result: BenchmarkResult):
        """çµæœè¿½åŠ ã¨ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°"""
        self.report.results.append(result)
        self.report.total_benchmarks += 1
        
        if result.winner == "python":
            self.report.python_wins += 1
        elif result.winner == "nodejs":
            self.report.nodejs_wins += 1
        else:
            self.report.ties += 1
    
    def _analyze_results(self):
        """çµæœåˆ†æ"""
        if not self.report.results:
            return
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        category_stats = {}
        for category in BenchmarkCategory:
            category_results = [r for r in self.report.results if r.category == category]
            if category_results:
                python_wins = sum(1 for r in category_results if r.winner == "python")
                nodejs_wins = sum(1 for r in category_results if r.winner == "nodejs")
                ties = sum(1 for r in category_results if r.winner == "tie")
                
                category_stats[category.value] = {
                    "total": len(category_results),
                    "python_wins": python_wins,
                    "nodejs_wins": nodejs_wins,
                    "ties": ties
                }
        
        # å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
        performance_ratios = [r.performance_ratio for r in self.report.results 
                             if r.performance_ratio > 0 and r.performance_ratio != 1.0]
        
        summary_stats = {
            "category_breakdown": category_stats,
            "overall_performance_ratio": statistics.mean(performance_ratios) if performance_ratios else 1.0,
            "nodejs_win_rate": self.report.nodejs_wins / self.report.total_benchmarks if self.report.total_benchmarks > 0 else 0,
            "python_win_rate": self.report.python_wins / self.report.total_benchmarks if self.report.total_benchmarks > 0 else 0
        }
        
        self.report.summary_stats = summary_stats
    
    def _generate_recommendations(self):
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        nodejs_win_rate = self.report.summary_stats.get("nodejs_win_rate", 0)
        python_win_rate = self.report.summary_stats.get("python_win_rate", 0)
        
        if nodejs_win_rate > python_win_rate + 0.2:  # 20%ä»¥ä¸Šã®å·®
            self.report.recommendations.append(
                "ğŸš€ Node.jsç‰ˆãŒç·åˆçš„ã«å„ªç§€ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç§»è¡Œã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )
        elif python_win_rate > nodejs_win_rate + 0.2:
            self.report.recommendations.append(
                "ğŸ Pythonç‰ˆãŒç·åˆçš„ã«å„ªç§€ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç¾è¡Œç¶­æŒã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )
        else:
            self.report.recommendations.append(
                "âš–ï¸ ä¸¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯åŒç­‰ãƒ¬ãƒ™ãƒ«ã§ã™ã€‚ä»–ã®è¦å› ã‚’è€ƒæ…®ã—ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚"
            )
        
        # å¿œç­”æ™‚é–“ã«é–¢ã™ã‚‹æ¨å¥¨äº‹é …
        response_time_results = [r for r in self.report.results 
                               if r.category == BenchmarkCategory.RESPONSE_TIME]
        if response_time_results:
            avg_ratio = statistics.mean(r.performance_ratio for r in response_time_results)
            if avg_ratio < 0.8:  # Node.jsç‰ˆãŒ20%ä»¥ä¸Šé«˜é€Ÿ
                self.report.recommendations.append(
                    "âš¡ Node.jsç‰ˆã®å¿œç­”æ™‚é–“ãŒå„ªç§€ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“å‘ä¸Šã®ãŸã‚ç§»è¡Œæ¤œè¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
                )
        
        # å®‰å®šæ€§ã«é–¢ã™ã‚‹æ¨å¥¨äº‹é …
        stability_results = [r for r in self.report.results 
                           if r.category == BenchmarkCategory.STABILITY]
        if stability_results:
            for result in stability_results:
                python_stability = result.python_metrics.get("success_rate", 0)
                nodejs_stability = result.nodejs_metrics.get("success_rate", 0)
                
                if min(python_stability, nodejs_stability) < 0.95:  # 95%æœªæº€ã¯å•é¡Œ
                    self.report.recommendations.append(
                        "âš ï¸ é•·æ™‚é–“ç¨¼åƒã§ã®å®‰å®šæ€§ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°èª¿æŸ»ãŒå¿…è¦ã§ã™ã€‚"
                    )
    
    def print_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("âš¡ Phase 9: Performance Benchmark Report - Python vs Node.js")
        print("=" * 80)
        
        print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œçµ±è¨ˆ:")
        print(f"  ç·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {self.report.total_benchmarks}")
        print(f"  Pythonå„ªä½: {self.report.python_wins}")
        print(f"  Node.jså„ªä½: {self.report.nodejs_wins}")
        print(f"  å¼•ãåˆ†ã‘: {self.report.ties}")
        
        if self.report.total_benchmarks > 0:
            nodejs_win_rate = (self.report.nodejs_wins / self.report.total_benchmarks) * 100
            python_win_rate = (self.report.python_wins / self.report.total_benchmarks) * 100
            tie_rate = (self.report.ties / self.report.total_benchmarks) * 100
            
            print(f"  Node.jså‹ç‡: {nodejs_win_rate:.1f}%")
            print(f"  Pythonå‹ç‡: {python_win_rate:.1f}%")
            print(f"  å¼•ãåˆ†ã‘ç‡: {tie_rate:.1f}%")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°çµæœ
        print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°çµæœ:")
        for category in BenchmarkCategory:
            category_results = [r for r in self.report.results if r.category == category]
            if category_results:
                print(f"\n  {category.value.upper()}:")
                for result in category_results:
                    winner_icon = "ğŸ¥‡" if result.winner != "tie" else "ğŸ¤"
                    print(f"    {winner_icon} {result.test_name}: {result.winner} (æ¯”ç‡: {result.performance_ratio:.2f})")
        
        # å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        if self.report.summary_stats:
            overall_ratio = self.report.summary_stats.get("overall_performance_ratio", 1.0)
            print(f"\nğŸ“ˆ å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")
            print(f"  å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”ç‡ (Node.js/Python): {overall_ratio:.2f}")
            
            if overall_ratio < 0.9:
                print("  â†’ Node.jsç‰ˆãŒç·åˆçš„ã«é«˜é€Ÿã§ã™")
            elif overall_ratio > 1.1:
                print("  â†’ Pythonç‰ˆãŒç·åˆçš„ã«é«˜é€Ÿã§ã™")
            else:
                print("  â†’ ä¸¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯åŒç­‰ã§ã™")
        
        # æ¨å¥¨äº‹é …
        if self.report.recommendations:
            print(f"\nğŸ’¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦³ç‚¹ã§ã®æ¨å¥¨äº‹é …:")
            for recommendation in self.report.recommendations:
                print(f"  {recommendation}")
        
        # æœ€çµ‚è©•ä¾¡
        print(f"\nğŸ† ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç·åˆè©•ä¾¡:")
        if self.report.nodejs_wins > self.report.python_wins:
            print("  ğŸš€ Node.jsç‰ˆãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é¢ã§å„ªä½ã§ã™")
        elif self.report.python_wins > self.report.nodejs_wins:
            print("  ğŸ Pythonç‰ˆãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é¢ã§å„ªä½ã§ã™")
        else:
            print("  âš–ï¸ ä¸¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯æ‹®æŠ—ã—ã¦ã„ã¾ã™")
        
        print("\n" + "=" * 80)
    
    def save_performance_charts(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆä¿å­˜"""
        try:
            # å¿œç­”æ™‚é–“æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ
            response_time_results = [r for r in self.report.results 
                                   if r.category == BenchmarkCategory.RESPONSE_TIME]
            
            if response_time_results:
                fig, ax = plt.subplots(figsize=(10, 6))
                
                test_names = [r.test_name for r in response_time_results]
                python_times = [r.python_metrics.get("mean", 0) for r in response_time_results]
                nodejs_times = [r.nodejs_metrics.get("mean", 0) for r in response_time_results]
                
                x = np.arange(len(test_names))
                width = 0.35
                
                ax.bar(x - width/2, python_times, width, label='Python', alpha=0.8)
                ax.bar(x + width/2, nodejs_times, width, label='Node.js', alpha=0.8)
                
                ax.set_xlabel('Test')
                ax.set_ylabel('Response Time (seconds)')
                ax.set_title('Response Time Comparison: Python vs Node.js')
                ax.set_xticks(x)
                ax.set_xticklabels(test_names, rotation=45)
                ax.legend()
                
                plt.tight_layout()
                plt.savefig("phase9_response_time_comparison.png")
                plt.close()
                
                print("ğŸ“Š å¿œç­”æ™‚é–“æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: phase9_response_time_comparison.png")
        
        except ImportError:
            print("âš ï¸ matplotlib ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒãƒ£ãƒ¼ãƒˆã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("âš¡ Phase 9: Performance Benchmark Tests")
    print("Pythonç‰ˆ vs Node.jsç‰ˆ è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
    print("=" * 80)
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–
    benchmark_suite = MCPPerformanceBenchmark()
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    report = await benchmark_suite.run_performance_benchmarks()
    
    # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
    benchmark_suite.print_performance_report()
    
    # ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ
    benchmark_suite.save_performance_charts()
    
    # JSONãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    json_report = {
        "performance_benchmark_report": {
            "python_wins": report.python_wins,
            "nodejs_wins": report.nodejs_wins,
            "ties": report.ties,
            "total_benchmarks": report.total_benchmarks,
            "summary_stats": report.summary_stats,
            "recommendations": report.recommendations,
            "benchmark_results": [
                {
                    "category": result.category.value,
                    "test_name": result.test_name,
                    "winner": result.winner,
                    "performance_ratio": result.performance_ratio,
                    "python_metrics": result.python_metrics,
                    "nodejs_metrics": result.nodejs_metrics
                }
                for result in report.results
            ]
        },
        "timestamp": datetime.now().isoformat(),
        "phase": "9",
        "test_type": "performance_benchmark"
    }
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    with open("phase9_performance_benchmark_report.json", "w", encoding="utf-8") as f:
        json.dump(json_report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: phase9_performance_benchmark_report.json")
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
    if report.nodejs_wins > report.python_wins:
        return 0  # Node.jsç‰ˆå„ªä½
    elif report.python_wins > report.nodejs_wins:
        return 1  # Pythonç‰ˆå„ªä½  
    else:
        return 2  # æ‹®æŠ—


if __name__ == "__main__":
    asyncio.run(main())